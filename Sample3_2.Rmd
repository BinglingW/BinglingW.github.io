---
title: "Predicting the Election Result Pt.2"
author: "Bingling"
date: "3/11/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidycensus)
library(rpart)
library(rpart.plot)
library(caret)
library(splitTools)
library(ipred)    # Bagging
library(ranger)   # Random Forest
library(gbm)      # Gradient Boosting
```
```{r census_data, include=FALSE}
census_api_key("c507a0514a354e5a4a9d73c1a9beeac207f0c807", overwrite = TRUE, install = TRUE)
readRenviron("~/.Renviron")
censusdata <- get_acs(geography = "county",
              variables = c(
                      less_than_highschool = "B06009_002",
                      race_white = "B02001_002",
                      race_black = "B02001_003",
                      race_asian = "B02001_005",
                      bachelor = "B06009_005",
                      income_1_1 = "B06010_004",
                      income_1_2 = "B06010_005",
                      income_1_3 = "B06010_006",
                      income_2_1 = "B06010_007",
                      income_2_2 = "B06010_008",
                      income_2_3 = "B06010_008",
                      income_3_1 = "B06010_009",
                      income_3_2 = "B06010_010",
                      income_3_3 = "B06010_011",
                      male_1819 = "B01001_007",
                      male_20 = "B01001_008",
                      male_2124 = "B01001_010",
                      male_2529 = "B01001_011",
                      male_4044 = "B01001_014",
                      male_4549 = "B01001_015",
                      male_6061 = "B01001_018",
                      male_6264 = "B01001_019",
                      male_6566 = "B01001_020",
                      male_6769 = "B01001_021",
                      female_1819 = "B01001_031",
                      female_20 = "B01001_032",
                      female_21 = "B01001_033",
                      female_2224 = "B01001_034",
                      female_2429 = "B01001_035",
                      female_4044 = "B01001_038",
                      female_4549 = "B01001_039",
                      female_6061 = "B01001_042",
                      female_6264 = "B01001_043",
                      female_6566 = "B01001_044",
                      female_6569 = "B01001_045",
                      own_children = "B09002_001",
                      veteran = "B21001_002",
                      english_only = "C16002_002",
                      moved_from_diff_state = "B07001_065",
                      below_poverty = "B29003_002",
                      total_population = "B01003_001"),
                      year = 2019)
```
```{r clean_data, include=FALSE}
acs_data <- censusdata %>%
  separate(NAME, into = c("county","state"), sep = ",") %>%
  pivot_wider(id_cols = c(state,county), names_from = variable, values_from = estimate) %>%
  mutate(PE_income_low = (income_1_1+income_1_2+income_1_3)/total_population,
         PE_income_middle = (income_2_1+income_2_2)/total_population,
         PE_income_high = (income_3_1+income_3_2+income_3_3)/total_population,
         PE_male_justadult = (male_1819)/total_population,
         PE_male_20 = (male_20+male_2124+male_2529)/total_population,
         PE_male_40 = (male_4044+male_4549)/total_population,
         PE_male_60 = (male_6061+male_6264+male_6566+male_6769)/total_population,
         PE_female_justadult = (female_1819)/total_population,
         PE_female_20 = (female_20+female_21+female_2224+female_2429)/total_population,
         PE_female_40 = (female_4044+female_4549)/total_population,
         PE_female_60 = (female_6061+female_6264+female_6566+female_6569)/total_population,
         PE_white = race_white/total_population,
         PE_black = race_black/total_population,
         PE_asian = race_asian/total_population,
         PE_veteran = veteran/total_population,
         PE_move_dffstate = moved_from_diff_state/total_population,
         PE_bachelor = bachelor/total_population,
         PE_lesshighschool = less_than_highschool/total_population,
         PE_own_children = own_children/total_population,
         PE_english_only = english_only/total_population) %>%
  select(county,
         state,
         PE_income_low,
         PE_income_middle,
         PE_income_high,
         PE_male_justadult,
         PE_male_20,
         PE_male_40,
         PE_male_60,
         PE_female_justadult,
         PE_female_20,
         PE_female_40,
         PE_female_60,
         PE_white,
         PE_black,
         PE_asian,
         PE_veteran,
         PE_move_dffstate,
         PE_english_only,
         PE_bachelor,
         PE_lesshighschool,
         PE_own_children 
         ) %>% 
  drop_na()
acs_data$state = str_trim(acs_data$state, side='both')
```
```{r election2, include=FALSE}
election <- read_csv("president_county_candidate.csv")
election %>%
  filter(won == TRUE) %>%
  select(state, county, candidate) -> winner
election %>%
  mutate(pctvote = 100*total_votes/sum(total_votes)) %>%
  pivot_wider(id_cols = c(state,county), names_from = c(candidate), values_from = pctvote) %>% 
  select(state, county, `Joe Biden`, `Donald Trump` ) %>%
  inner_join(winner, by = c("state", "county")) %>%
  inner_join(acs_data, by=c("state","county")) %>%
  mutate(winner = if_else(candidate == "Joe Biden", 1L, 0L)) %>%
  mutate(candidate = as.factor(candidate), winner = as.factor(winner)) -> election2
election2
```

\

## 1. Decision Tree

The decision tree can be interpreted as the following:   

- **Layer 1**: The overal probability of Trump being the winner is **16%**.
- **Layer 2**: *93% of counties have more than 53% of people as white people*. If the percentage of white people is more than 53% in the county, the probability of Trump being the winner is **11%**. If the percentage is less than 53%, the probability of Biden being the winner is **84%**.
- **Layer 2**: Among the counties with more than 53% of people as white people, if the county has less than 2.8% of people as Asians, the probability of Trump being the winner is **6%**.*84% of county meet this if-condition*.  If the county has more than 2.8% of people as Asians, the probability of Biden being the winner is **60%**. *9% of county meet this if-condition*.
- **Layer 3**: Among the counties with more than 53% white people and less than 2.8% Asians, if there are more than 8% of families who speak only English at home, the probability of Trump being the winner is **5%**. *84% of county meet this if-condition*. If there are less than 8% of families who speak only English at home, the probability of Biden being the winner is **78%**. *1% of county meet this if-condition*.
- **Layer 3**: Among the counties with more than 53% white people and more than 2.8% Asians, if less than 12% of popolation has a bachelor degree, the probability of Trump being the winner is **27%**. *2% of county meet this if-condition*. If more than 12% of population has a bachelor degree, the probability of Biden being the winner is **72%**. *7% of county meet this if-condition*.
- **Layer 4**: Among the counties with more than 53% white people, less than 2.8% Asians, and less than 12% of popolation who has a bachelor degree, if less than 18% of population has a bachelor degree, the probability of Trump being the winner is **4%**. *82% of county meet this if-condition*.If more than 18% of population has a bachelor degree, the probability of Biden being the winner is **65%**. *1% of county meet this if-condition*.
- **Layer 4**: Among the counties with more than 53% white people, more than 2.8% Asians, and more than 12% hain ga bachelor degree, if the percentage of white people in the county is more than 86%, the probability of Trump being the winner is **27**. *1% of county meet this if-condition*. If the percentage of white people in the county is less than 86%, the probability of Biden being the winner is **83%**. *5% of county meet this if-condition*.

```{r split_data}
# split data
set.seed(3451)
data_split <- partition(election2$winner, p = c(train = 0.8, test = 0.2))
train <- election2[data_split$train, ]
test <- election2[data_split$test, ]
```
```{r tree}
# create a tree
model1 <- rpart(candidate ~ PE_bachelor + PE_lesshighschool + PE_veteran + PE_white + 
                  PE_asian + PE_male_60 + PE_female_60 + PE_female_40 + PE_female_justadult +
                  PE_male_justadult + PE_income_middle + PE_income_high +
                  PE_income_low + PE_english_only, 
                data = train,
                method = "class",
                control = rpart.control(cp = 0.015))
rpart.plot(model1, yesno = 2, type = 1)
```

\

# 2. Bagging

The quality of prediction can improve because:   

1. Bagging uses the ensemble method instead of a single tree. Because of the aggregation process, bagging is able to reduce the high variance and potentially large prediction error that a single decision tree has. 

2. It can also eliminate the overfitting of models through reducing the variance since bagging introduces a random component into the tree building process through bootstrapping.

The drawbacks of this method include:

1. This method cannot increase much predictive power for algorithms that are more stable or have high bias (i.e. low variability), such as linear regressions.

2. It can be computationally expensive.

3. There can be a loss of interpretability of a model.

4. It can result in tree correlation, which limits the effect of variance reduction.

```{r}
set.seed(123)
model2 <- bagging(
  formula = candidate ~ PE_bachelor + PE_lesshighschool + PE_veteran + PE_white + 
                  PE_asian + PE_male_60 + PE_female_60 + PE_female_40 + PE_female_justadult +
                  PE_male_justadult + PE_income_middle + PE_income_high +
                  PE_income_low + PE_english_only,
  data = train,
  nbagg = 100,
  coob = TRUE,
  control = rpart.control(minsplit=2, cp=0)
)
print(model2)
```

\

# 3. Random Forests

The OOB's rmse is 0.2378, and the Random Forest's rmse is 0.2292, which is smaller. So this method of Random Forests has a slightly stronger predictive performance.

The advantages of random forests include:     

1. It reduces tree correlation by injecting more randomness into the tree-growing process by creating a more diverse set of trees.

2. This method does not have a large variablity in its prediction accuracy when tuning.

The disadvantages of random forests include:       

1. There are multiple hyperparameters that should be manually set.

2. Adding more hypoerparameters can be computationally expensive.
```{r}
n_features <- length(setdiff(names(train), "candidate"))
model3 <- ranger(
  candidate ~ PE_bachelor + PE_lesshighschool + PE_veteran + PE_white + 
                  PE_asian + PE_male_60 + PE_female_60 + PE_female_40 + PE_female_justadult +
                  PE_male_justadult + PE_income_middle + PE_income_high +
                  PE_income_low + PE_english_only,
  data = train,
  mtry = floor(n_features / 3),
  respect.unordered.factors = "order",
  seed = 123
)
model3_rmse <- sqrt(model3$prediction.error)
print(model3)
```

\

# 4. Construct confusion metrics for test data
```{r}
# single decision tree
predict_model1_test <- predict(model1, test, type = 'class')
confusionMatrix(predict_model1_test, test$candidate)
```
```{r}
# bagging
predict_model2_test <- predict(model2, test, type="class")
confusionMatrix(predict_model2_test, test$candidate)
```
```{r}
# random forest
predict_model3_test <- predict(model3, test, type="response",se.method="infjack" )
confusionMatrix(predict_model3_test$predictions, test$candidate)
```

\

# 5. Construct confusion matrices for train data

The predictions using train data have a higher accuracy than predictions using test data because the models were built using the train data.
```{r}
# single decision tree
predict_model1_train <- predict(model1, train, type = 'class')
confusionMatrix(predict_model1_train, train$candidate)
```
```{r}
# bagging
predict_model2_train <- predict(model2, train, type="class")
confusionMatrix(predict_model2_train, train$candidate)
```
```{r}
# random forest
predict_model3_train <- predict(model3, train, type="response",se.method="infjack" )
table(train$candidate, predict_model3_train$predictions)
```

